# -*- coding: utf-8 -*-
"""Assignment5_700745497

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cwUgWTeSOjKmlgm1pVQPXi_xGeZxv9LF
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
from sklearn import preprocessing, metrics
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
sns.set(style="white", color_codes=True)
import warnings
warnings.filterwarnings("ignore")

df= pd.read_csv(r"CC GENERAL.csv")
df.head()

df['TENURE'].value_counts()
df.fillna(df.mean(), inplace=True)#replacing the null/Empty values with the mean

X = df.drop('TENURE',axis=1).values
y = df['TENURE'].values

#Getting the shape of X and Y
x= df.iloc[:,1:-1]
y = df.iloc[:,-1]
print(x.shape,y.shape)

#a. Apply PCA on CC dataset.
pca = PCA(3)
x_pca = pca.fit_transform(x)
principalDf = pd.DataFrame(data = x_pca, columns = ['principal component 1', 'principal component 2', 'principal component 3'])
finalDf = pd.concat([principalDf, df.iloc[:,-1]], axis = 1)
finalDf.head()

#Apply k-means algorithm on the PCA result and report your observation if the silhouette score has
#improved or not?
X = finalDf.iloc[:,0:-1]
y = finalDf.iloc[:,-1]

nclusters = 3 # this is the k in kmeans
km = KMeans(n_clusters=nclusters)
km.fit(X)


y_cluster_kmeans = km.predict(X)

print(classification_report(y, y_cluster_kmeans, zero_division=1))
print(confusion_matrix(y, y_cluster_kmeans))

#finding the accuracy
train_accuracy = accuracy_score(y, y_cluster_kmeans)
print("\nAccuracy for our Training dataset with PCA:", train_accuracy)

#Calculating silhouette Score
Silscore = metrics.silhouette_score(X, y_cluster_kmeans)
print("Sihouette Score: ",Silscore)

#Perform Scaling+PCA+K-Means and report performance.
x = df.iloc[:,1:-1]
y = df.iloc[:,-1]
print(x.shape,y.shape)

# Scale the dataset
scaler = StandardScaler()
scaler.fit(x)
X_scaled_array = scaler.transform(x)

# Instantiate PCA
pca = PCA(3)
x_pca = pca.fit_transform(X_scaled_array)
principalDf = pd.DataFrame(data = x_pca, columns = ['principal component 1', 'principal component 2','principal component 3'])
finalDf = pd.concat([principalDf, df.iloc[:,-1]], axis = 1)
finalDf.head()

# this is the k in kmeans
x = finalDf.iloc[:,0:-1]
y = finalDf["TENURE"]
print(X.shape,y.shape)

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.34,random_state=0)
nclusters = 3 
km = KMeans(n_clusters=nclusters)
km.fit(X_train,y_train)

# predict the cluster for each training data point
y_clus_train = km.predict(X_train)

# Summary of the predictions made by the classifier
print(classification_report(y_train, y_clus_train, zero_division=1))
print(confusion_matrix(y_train, y_clus_train))

train_accuracy = accuracy_score(y_train, y_clus_train)
print("Accuracy for our Training dataset with PCA:", train_accuracy)

#Calculating sihouette Score
score = metrics.silhouette_score(X_train, y_clus_train)
print("Sihouette Score: ",score)   #ranges from -1 to +1, high value shows that it is matched more

# predict the cluster for each testing data point
y_clus_test = km.predict(X_test)

# Summary of the predictions made by the classifier
print(classification_report(y_test, y_clus_test, zero_division=1))
print(confusion_matrix(y_test, y_clus_test))

train_accuracy = accuracy_score(y_test, y_clus_test)
print("\nAccuracy for our Training dataset with PCA:", train_accuracy)

#Calculating sihouette Score
score = metrics.silhouette_score(X_test, y_clus_test)
print("Sihouette Score: ",score)   #ranges from -1 to +1, high value shows that it is matched more

#2. Use pd_speech_features.csv
#a. Perform Scaling
#b. Apply PCA (k=3)
#c. Use SVM to report performance

df_pd = pd.read_csv(r"pd_speech_features.csv")

df_pd.isnull().any()
X = df_pd.drop('class',axis=1).values
Y = df_pd['class'].values

#Perform Scaling
scaler = StandardScaler()
X_Scale = scaler.fit_transform(X)

#Apply PCA (k=3)
pca3 = PCA(n_components=3)
principalComponents = pca3.fit_transform(X_Scale)

principalDf = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2','Principal Component 3'])

finalDf = pd.concat([principalDf, df_pd[['class']]], axis = 1)
finalDf.head()

X = finalDf.drop('class',axis=1).values
Y = finalDf['class'].values
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3,random_state=0)

#Use SVM to report performance
from sklearn.svm import SVC

svmClassifier = SVC()
svmClassifier.fit(X_train, Y_train)

y_pred = svmClassifier.predict(X_test)

# Summary of the predictions made by the classifier
print(classification_report(Y_test, y_pred, zero_division=1))
print(confusion_matrix(Y_test, y_pred))
# Accuracy score
glass_acc_svc = accuracy_score(y_pred,Y_test)
print('accuracy is',glass_acc_svc)

#Calculate sihouette Score
score = metrics.silhouette_score(X_test, y_pred)
print("Sihouette Score: ",score)

#3. Apply Linear Discriminant Analysis (LDA) on Iris.csv dataset to reduce dimensionality of data tok=2.

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
df_iris = pd.read_csv(r"Iris.csv")

df_iris.isnull().any()

x = df_iris.iloc[:,1:-1]
y = df_iris.iloc[:,-1]
print(x.shape,y.shape)

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)
le = LabelEncoder()
y = le.fit_transform(y)

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=2)
X_train = lda.fit_transform(X_train, y_train)
X_test = lda.transform(X_test)
print(X_train.shape,X_test.shape)



#Briefly identify the difference between PCA and LDA
PCA(Principal Component Analysis):
  2.PCA is an unsupervised algorithm that does not care about classes and labels and only aims to find the principal 
  components to maximize the variance in the given dataset.
  3.PCA is assumed to be an as good performer for a comparatively small sample size.
LDA(Linear Discriminant Analysis):
  1.LDA is a supervised algorithm that aims to find the linear discriminants to represent the axes that maximize 
  separation between different classes of data.
  2.suitable for multi-class classification tasks.